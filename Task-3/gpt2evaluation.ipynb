{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import GPT2ForSequenceClassification, Trainer, GPT2Config, GPT2Tokenizer, TrainingArguments","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:05.230995Z","iopub.execute_input":"2023-08-06T17:52:05.231471Z","iopub.status.idle":"2023-08-06T17:52:30.498190Z","shell.execute_reply.started":"2023-08-06T17:52:05.231434Z","shell.execute_reply":"2023-08-06T17:52:30.497088Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:30.500380Z","iopub.execute_input":"2023-08-06T17:52:30.501150Z","iopub.status.idle":"2023-08-06T17:52:30.505942Z","shell.execute_reply.started":"2023-08-06T17:52:30.501118Z","shell.execute_reply":"2023-08-06T17:52:30.504930Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:30.507591Z","iopub.execute_input":"2023-08-06T17:52:30.508382Z","iopub.status.idle":"2023-08-06T17:52:30.519134Z","shell.execute_reply.started":"2023-08-06T17:52:30.508344Z","shell.execute_reply":"2023-08-06T17:52:30.518153Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"config = GPT2Config.from_json_file('/kaggle/input/gpt2-trained-weights/config.json')\nmodel = GPT2ForSequenceClassification(config)\nmodel.load_state_dict(torch.load('/kaggle/input/gpt2-trained-weights/pytorch_model.bin'))","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:30.522249Z","iopub.execute_input":"2023-08-06T17:52:30.522603Z","iopub.status.idle":"2023-08-06T17:52:46.329066Z","shell.execute_reply.started":"2023-08-06T17:52:30.522572Z","shell.execute_reply":"2023-08-06T17:52:46.327995Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:46.330801Z","iopub.execute_input":"2023-08-06T17:52:46.331182Z","iopub.status.idle":"2023-08-06T17:52:47.439284Z","shell.execute_reply.started":"2023-08-06T17:52:46.331149Z","shell.execute_reply":"2023-08-06T17:52:47.438225Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1297ce5a19941c7bfc9b8e7bc1b7540"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c064c19570b3487b928b7fd7ca32dfa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0324cdc3564445709c5fc3ff915450f3"}},"metadata":{}}]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:47.440981Z","iopub.execute_input":"2023-08-06T17:52:47.441371Z","iopub.status.idle":"2023-08-06T17:52:47.451545Z","shell.execute_reply.started":"2023-08-06T17:52:47.441335Z","shell.execute_reply":"2023-08-06T17:52:47.450453Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"GPT2ForSequenceClassification(\n  (transformer): GPT2Model(\n    (wte): Embedding(50258, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=768, out_features=5, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"X_train = pd.read_csv('/kaggle/input/splitted-dataset/Dataset/Dataset/X_train.csv')\ny_train = pd.read_csv('/kaggle/input/splitted-dataset/Dataset/Dataset/y_train.csv')\nX_test = pd.read_csv('/kaggle/input/splitted-dataset/Dataset/Dataset/X_test.csv')\ny_test =pd.read_csv('/kaggle/input/splitted-dataset/Dataset/Dataset/y_test.csv')\nX_train_discarded = pd.read_csv('/kaggle/input/splitted-dataset/Dataset/Dataset/X_train_discarded.csv')\ny_train_discarded = pd.read_csv('/kaggle/input/splitted-dataset/Dataset/Dataset/y_train_discarded.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:47.453259Z","iopub.execute_input":"2023-08-06T17:52:47.453614Z","iopub.status.idle":"2023-08-06T17:52:49.928404Z","shell.execute_reply.started":"2023-08-06T17:52:47.453582Z","shell.execute_reply":"2023-08-06T17:52:49.927075Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.drop('Unnamed: 0', axis=1)\ny_train = y_train.drop('Unnamed: 0', axis=1)\nX_test = X_test.drop('Unnamed: 0', axis=1)\ny_test = y_test.drop('Unnamed: 0', axis=1)\nX_train_discarded = X_train_discarded.drop('Unnamed: 0', axis=1)\ny_train_discarded = y_train_discarded.drop('Unnamed: 0', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:49.929806Z","iopub.execute_input":"2023-08-06T17:52:49.930224Z","iopub.status.idle":"2023-08-06T17:52:49.988157Z","shell.execute_reply.started":"2023-08-06T17:52:49.930175Z","shell.execute_reply":"2023-08-06T17:52:49.987182Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# GPT-2 tokenizer does not have a padding token by default. We need to add one.\n# We add a pad token and also update the model config.\ntokenizer.add_special_tokens({'pad_token': '<PAD>'})\ntokenizer.model_max_length = 512\n\n# Tokenize the text data\ntest_encodings = tokenizer(X_test['Text'].tolist(), truncation=True, padding=True, max_length=512)\n\n# Convert labels to integer values\nle = LabelEncoder()\ntest_labels = le.fit_transform(y_test['PICO_Entity'])\n\n# Prepare the dataset\nclass MyDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntest_dataset = MyDataset(test_encodings, test_labels)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:50.001972Z","iopub.execute_input":"2023-08-06T17:52:50.002301Z","iopub.status.idle":"2023-08-06T17:52:50.612064Z","shell.execute_reply.started":"2023-08-06T17:52:50.002276Z","shell.execute_reply":"2023-08-06T17:52:50.611062Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom sklearn.metrics import matthews_corrcoef, cohen_kappa_score\nfrom scipy.stats import pointbiserialr, pearsonr, spearmanr, kendalltau\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n\n    precision = precision_score(labels, predictions, average='weighted')\n    recall = recall_score(labels, predictions, average='weighted')\n    f1 = f1_score(labels, predictions, average='weighted')\n    accuracy = accuracy_score(labels, predictions)\n    kappa = cohen_kappa_score(labels, predictions)\n    conf_matrix = confusion_matrix(labels, predictions)\n\n    mcc = []\n    pearson = []\n    spearman = []\n    kendall = []\n    point_biserial = []\n    for class_index in range(logits.shape[1]): # iterate over classes\n        binary_labels = (labels == class_index)\n        binary_predictions = (predictions == class_index)\n\n        mcc.append(matthews_corrcoef(binary_labels, binary_predictions))\n        pearson.append(pearsonr(binary_labels, binary_predictions)[0])\n        spearman.append(spearmanr(binary_labels, binary_predictions)[0])\n        kendall.append(kendalltau(binary_labels, binary_predictions)[0])\n        point_biserial.append(pointbiserialr(binary_labels, binary_predictions)[0])\n\n    return {\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'accuracy': accuracy,\n        'kappa': kappa,\n        'confusion_matrix': conf_matrix,\n        'mcc': np.mean(mcc),  # Take the mean of the per-class MCCs\n        'pearson': np.mean(pearson),  # Take the mean of the per-class Pearson coefficients\n        'spearman': np.mean(spearman),  # Take the mean of the per-class Spearman coefficients\n        'kendall': np.mean(kendall),  # Take the mean of the per-class Kendall coefficients\n        'point_biserial': np.mean(point_biserial),  # Take the mean of the per-class Point Biserial coefficients\n    }","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:50.613532Z","iopub.execute_input":"2023-08-06T17:52:50.614110Z","iopub.status.idle":"2023-08-06T17:52:50.626805Z","shell.execute_reply.started":"2023-08-06T17:52:50.614074Z","shell.execute_reply":"2023-08-06T17:52:50.625923Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',  # Output directory\n    num_train_epochs=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_steps=3000,  # Save every 1000 steps\n)\n# Create the trainer and fine-tune the model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:50.628249Z","iopub.execute_input":"2023-08-06T17:52:50.628692Z","iopub.status.idle":"2023-08-06T17:52:50.814452Z","shell.execute_reply.started":"2023-08-06T17:52:50.628631Z","shell.execute_reply":"2023-08-06T17:52:50.813443Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\neval_results = trainer.evaluate(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:52:50.815994Z","iopub.execute_input":"2023-08-06T17:52:50.816389Z","iopub.status.idle":"2023-08-06T17:53:45.421109Z","shell.execute_reply.started":"2023-08-06T17:52:50.816353Z","shell.execute_reply":"2023-08-06T17:53:45.420207Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8/8 00:51]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"[[ 44   3   0   0  22]\n [  0  54   0   0  15]\n [  0   0  16   1   8]\n [  0   0   0   2   3]\n [ 22  11  11   0 788]]\" of type <class 'numpy.ndarray'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230806_175314-73vyra7t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lukecage/huggingface/runs/73vyra7t' target=\"_blank\">earnest-sun-29</a></strong> to <a href='https://wandb.ai/lukecage/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lukecage/huggingface' target=\"_blank\">https://wandb.ai/lukecage/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lukecage/huggingface/runs/73vyra7t' target=\"_blank\">https://wandb.ai/lukecage/huggingface/runs/73vyra7t</a>"},"metadata":{}}]},{"cell_type":"code","source":"eval_results = trainer.evaluate(test_dataset)\nprint(eval_results)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:53:45.425454Z","iopub.execute_input":"2023-08-06T17:53:45.428104Z","iopub.status.idle":"2023-08-06T17:53:50.321203Z","shell.execute_reply.started":"2023-08-06T17:53:45.428067Z","shell.execute_reply":"2023-08-06T17:53:50.320245Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"[[ 44   3   0   0  22]\n [  0  54   0   0  15]\n [  0   0  16   1   8]\n [  0   0   0   2   3]\n [ 22  11  11   0 788]]\" of type <class 'numpy.ndarray'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.2467719167470932, 'eval_precision': 0.9031719308669772, 'eval_recall': 0.904, 'eval_f1': 0.9033668654969342, 'eval_accuracy': 0.904, 'eval_kappa': 0.6740370511218559, 'eval_confusion_matrix': array([[ 44,   3,   0,   0,  22],\n       [  0,  54,   0,   0,  15],\n       [  0,   0,  16,   1,   8],\n       [  0,   0,   0,   2,   3],\n       [ 22,  11,  11,   0, 788]]), 'eval_mcc': 0.6375251137269544, 'eval_pearson': 0.6375251137269542, 'eval_spearman': 0.6375251137269544, 'eval_kendall': 0.6375251137269544, 'eval_point_biserial': 0.6375251137269542, 'eval_runtime': 4.8752, 'eval_samples_per_second': 205.122, 'eval_steps_per_second': 1.641}\n","output_type":"stream"}]}]}