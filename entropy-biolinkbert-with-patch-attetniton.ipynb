{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6939553,"sourceType":"datasetVersion","datasetId":3985277}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import BertModel, BertTokenizer\n\n# Define the Siamese BERT Network\nclass SiameseBertNetwork(nn.Module):\n    def __init__(self):\n        super(SiameseBertNetwork, self).__init__()\n        self.bert = BertModel.from_pretrained('michiyasunaga/BioLinkBERT-large')\n        self.similarity_layer = nn.Sequential(\n            nn.Linear(self.bert.config.hidden_size * 2, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n        pooled_output1 = self.bert(input_ids=input_ids1, attention_mask=attention_mask1).pooler_output\n        pooled_output2 = self.bert(input_ids=input_ids2, attention_mask=attention_mask2).pooler_output\n        combined_output = torch.cat((pooled_output1, pooled_output2), 1)\n        similarity_score = self.similarity_layer(combined_output)\n        return similarity_score\n\n# Function to tokenize sentences\ndef tokenize(sentences, tokenizer):\n    return tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n\n# Initialize tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('michiyasunaga/BioLinkBERT-large')\nmodel = SiameseBertNetwork()","metadata":{"_uuid":"4b47dd8c-28f8-4c79-8e5d-517175fed96f","_cell_guid":"88916552-b945-4b6a-9e2b-d8153333c14e","collapsed":false,"execution":{"iopub.status.busy":"2023-12-20T19:56:59.532856Z","iopub.execute_input":"2023-12-20T19:56:59.534098Z","iopub.status.idle":"2023-12-20T19:57:00.790649Z","shell.execute_reply.started":"2023-12-20T19:56:59.534050Z","shell.execute_reply":"2023-12-20T19:57:00.789414Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nfile_path = '/kaggle/input/complete-dataset/summarized_abstracts_bert/summarized_patient-number2-articles.csv'\ndf = pd.read_csv(file_path)","metadata":{"_uuid":"98495a9a-225f-455e-b1d8-4cb824d431bc","_cell_guid":"7814d782-6f86-47fa-9b55-76b5bff26122","collapsed":false,"execution":{"iopub.status.busy":"2023-12-20T19:57:00.792652Z","iopub.execute_input":"2023-12-20T19:57:00.793176Z","iopub.status.idle":"2023-12-20T19:57:00.804493Z","shell.execute_reply.started":"2023-12-20T19:57:00.793141Z","shell.execute_reply":"2023-12-20T19:57:00.802691Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize\nimport pandas as pd\nimport string\n\n# Ensure you have the NLTK punkt tokenizer downloaded in your environment\nnltk.download('punkt')\n\n# Function to split text into sentences and remove punctuation\ndef split_into_sentences(text):\n    return sent_tokenize(text) if pd.notna(text) else []\n\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\n# Load your CSV file\nfile_path = '/kaggle/input/complete-dataset/summarized_abstracts_bert/summarized_patient-number2-articles.csv'  # Update with your file path\ndf = pd.read_csv(file_path)\n\n# Process the summaries to split into sentences and remove punctuation\narticles_sentences_cleaned = []\nfor summary in df['Summary']:\n    if pd.notna(summary):\n        sentences = split_into_sentences(summary)\n        cleaned_sentences = [remove_punctuation(sentence) for sentence in sentences]\n        articles_sentences_cleaned.append(cleaned_sentences)","metadata":{"_uuid":"86125b41-ea15-4c6f-a077-4defe69ae72f","_cell_guid":"8fc44e1f-7e8e-4d7d-a97b-34ab57b73a1a","collapsed":false,"execution":{"iopub.status.busy":"2023-12-20T19:57:00.807182Z","iopub.execute_input":"2023-12-20T19:57:00.807643Z","iopub.status.idle":"2023-12-20T19:57:00.823186Z","shell.execute_reply.started":"2023-12-20T19:57:00.807604Z","shell.execute_reply":"2023-12-20T19:57:00.821970Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"articles_sentences_cleaned[0]","metadata":{"_uuid":"8c299e62-91d1-41ab-aed4-557390860461","_cell_guid":"b960de17-401b-46ff-9174-d2817716906c","collapsed":false,"execution":{"iopub.status.busy":"2023-12-20T19:57:00.826033Z","iopub.execute_input":"2023-12-20T19:57:00.826452Z","iopub.status.idle":"2023-12-20T19:57:00.837140Z","shell.execute_reply.started":"2023-12-20T19:57:00.826410Z","shell.execute_reply":"2023-12-20T19:57:00.835922Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,4):\n    patient_history_sentences = [\n        \"65\", \"old woman\", \"arrives\",  \"to the ed\", \"medical history\", \"includes hypertension\", \"Atherosclerosis\", \"artery disease\", \"artery bypass\", \"when leaning forward\", \"heard on auscultation\", \"ECG\", \"global\"\n    ]\n\n    pubmed_articles = articles_sentences_cleaned[i]\n    # Tokenize sentences\n    tokenized_history = tokenize(patient_history_sentences, tokenizer)\n    tokenized_articles = tokenize(pubmed_articles, tokenizer)\n\n    # Compute similarity scores\n    with torch.no_grad():\n        all_scores = []\n        for i in range(len(tokenized_history['input_ids'])):\n            scores = []\n            for j in range(len(tokenized_articles['input_ids'])):\n                score = model(\n                    input_ids1=tokenized_history['input_ids'][i].unsqueeze(0),\n                    attention_mask1=tokenized_history['attention_mask'][i].unsqueeze(0),\n                    input_ids2=tokenized_articles['input_ids'][j].unsqueeze(0),\n                    attention_mask2=tokenized_articles['attention_mask'][j].unsqueeze(0)\n                )\n                scores.append(score.item())\n            all_scores.append(scores)\n\n    # Aggregate scores (maximum similarity for each sentence in the history list)\n    max_similarity_per_sentence = [max(scores) for scores in all_scores]\n    overall_entropy = sum(max_similarity_per_sentence) / len(max_similarity_per_sentence)\n\n    print(f\"Overall entropy between patient history and pubmed article: {overall_entropy}\")","metadata":{"_uuid":"016601b1-b349-4552-b437-cdc26aaeb3c2","_cell_guid":"f331a589-905c-4d0c-9846-d09351aa90bb","collapsed":false,"execution":{"iopub.status.busy":"2023-12-20T19:57:00.839111Z","iopub.execute_input":"2023-12-20T19:57:00.839521Z","iopub.status.idle":"2023-12-20T19:58:03.953729Z","shell.execute_reply.started":"2023-12-20T19:57:00.839488Z","shell.execute_reply":"2023-12-20T19:58:03.951856Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}