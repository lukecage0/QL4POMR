{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6939553,"sourceType":"datasetVersion","datasetId":3985277},{"sourceId":7234309,"sourceType":"datasetVersion","datasetId":4189170}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import BertModel, BertTokenizer\n\n# Define the Siamese BERT Network\nclass SiameseBertNetwork(nn.Module):\n    def __init__(self):\n        super(SiameseBertNetwork, self).__init__()\n        self.bert = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n        self.similarity_layer = nn.Sequential(\n            nn.Linear(self.bert.config.hidden_size * 2, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n        pooled_output1 = self.bert(input_ids=input_ids1, attention_mask=attention_mask1).pooler_output\n        pooled_output2 = self.bert(input_ids=input_ids2, attention_mask=attention_mask2).pooler_output\n        combined_output = torch.cat((pooled_output1, pooled_output2), 1)\n        similarity_score = self.similarity_layer(combined_output)\n        return similarity_score\n\n# Function to tokenize sentences\ndef tokenize(sentences, tokenizer):\n    return tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n\n# Initialize tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\nmodel = SiameseBertNetwork()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-19T02:24:19.433695Z","iopub.execute_input":"2023-12-19T02:24:19.434604Z","iopub.status.idle":"2023-12-19T02:24:20.268072Z","shell.execute_reply.started":"2023-12-19T02:24:19.434564Z","shell.execute_reply":"2023-12-19T02:24:20.266911Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nfile_path = '/kaggle/input/dataset/summarized_abstracts_bert/summarized_patient-number2-articles.csv'\ndf = pd.read_csv(file_path)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T02:24:20.269832Z","iopub.execute_input":"2023-12-19T02:24:20.270136Z","iopub.status.idle":"2023-12-19T02:24:20.279562Z","shell.execute_reply.started":"2023-12-19T02:24:20.270110Z","shell.execute_reply":"2023-12-19T02:24:20.278760Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize\nimport pandas as pd\nimport string\n\n# Ensure you have the NLTK punkt tokenizer downloaded in your environment\nnltk.download('punkt')\n\n# Function to split text into sentences and remove punctuation\ndef split_into_sentences(text):\n    return sent_tokenize(text) if pd.notna(text) else []\n\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\n# Load your CSV file\nfile_path = '/kaggle/input/dataset/summarized_abstracts_bert/summarized_patient-number2-articles.csv'  # Update with your file path\ndf = pd.read_csv(file_path)\n\n# Process the summaries to split into sentences and remove punctuation\narticles_sentences_cleaned = []\nfor summary in df['Summary']:\n    if pd.notna(summary):\n        sentences = split_into_sentences(summary)\n        cleaned_sentences = [remove_punctuation(sentence) for sentence in sentences]\n        articles_sentences_cleaned.append(cleaned_sentences)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T02:24:20.280751Z","iopub.execute_input":"2023-12-19T02:24:20.281563Z","iopub.status.idle":"2023-12-19T02:24:20.292847Z","shell.execute_reply.started":"2023-12-19T02:24:20.281534Z","shell.execute_reply":"2023-12-19T02:24:20.291706Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"articles_sentences_cleaned[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-19T02:24:20.295094Z","iopub.execute_input":"2023-12-19T02:24:20.295542Z","iopub.status.idle":"2023-12-19T02:24:20.305369Z","shell.execute_reply.started":"2023-12-19T02:24:20.295508Z","shell.execute_reply":"2023-12-19T02:24:20.304204Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['However the followup coronary angiography showed negative result and the symptom improved dramatically with the treatment of nonsteroidal antiinflammatory drug treatment',\n 'Therefore it is important for the clinician to differentiate acute myocardial infarctionacute stent thrombosis from this rare complication after PCI']"},"metadata":{}}]},{"cell_type":"code","source":"patient_history_sentences = [\n        \"65\", \"old woman\", \"arrives  to the ed\", \"medical history includes hypertension\", \"Atherosclerosis\", \"artery disease\", \"artery bypass\", \"when leaning forward\", \"heard on auscultation\", \"ECG\", \"global\"\n    ]\nfor i in range(0,4):\n    \n\n    pubmed_articles = articles_sentences_cleaned[i]\n    # Tokenize sentences\n    tokenized_history = tokenize(patient_history_sentences, tokenizer)\n    tokenized_articles = tokenize(pubmed_articles, tokenizer)\n\n    # Compute similarity scores\n    with torch.no_grad():\n        all_scores = []\n        for i in range(len(tokenized_history['input_ids'])):\n            scores = []\n            for j in range(len(tokenized_articles['input_ids'])):\n                score = model(\n                    input_ids1=tokenized_history['input_ids'][i].unsqueeze(0),\n                    attention_mask1=tokenized_history['attention_mask'][i].unsqueeze(0),\n                    input_ids2=tokenized_articles['input_ids'][j].unsqueeze(0),\n                    attention_mask2=tokenized_articles['attention_mask'][j].unsqueeze(0)\n                )\n                scores.append(score.item())\n            all_scores.append(scores)\n\n    # Aggregate scores (maximum similarity for each sentence in the history list)\n    max_similarity_per_sentence = [max(scores) for scores in all_scores]\n    overall_entropy = sum(max_similarity_per_sentence) / len(max_similarity_per_sentence)\n\n    print(f\"Overall entropy between patient history and pubmed article: {overall_entropy}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T02:24:20.306816Z","iopub.execute_input":"2023-12-19T02:24:20.307540Z","iopub.status.idle":"2023-12-19T02:24:37.321795Z","shell.execute_reply.started":"2023-12-19T02:24:20.307482Z","shell.execute_reply":"2023-12-19T02:24:37.320552Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Overall entropy between patient history and pubmed article: 0.43397176494965184\nOverall entropy between patient history and pubmed article: 0.4577015477877397\nOverall entropy between patient history and pubmed article: 0.4434421222943526\nOverall entropy between patient history and pubmed article: 0.4578437598852011\n","output_type":"stream"}]}]}