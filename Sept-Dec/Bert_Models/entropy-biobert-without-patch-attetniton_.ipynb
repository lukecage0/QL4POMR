{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import BertModel, BertTokenizer\n\n# Define the Siamese BERT Network\nclass SiameseBertNetwork(nn.Module):\n    def __init__(self):\n        super(SiameseBertNetwork, self).__init__()\n        self.bert = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n        self.similarity_layer = nn.Sequential(\n            nn.Linear(self.bert.config.hidden_size * 2, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n        pooled_output1 = self.bert(input_ids=input_ids1, attention_mask=attention_mask1).pooler_output\n        pooled_output2 = self.bert(input_ids=input_ids2, attention_mask=attention_mask2).pooler_output\n        combined_output = torch.cat((pooled_output1, pooled_output2), 1)\n        similarity_score = self.similarity_layer(combined_output)\n        return similarity_score\n\n# Function to tokenize sentences\ndef tokenize(sentences, tokenizer):\n    return tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n\n# Initialize tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\nmodel = SiameseBertNetwork()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-16T20:49:29.118816Z","iopub.execute_input":"2023-12-16T20:49:29.119299Z","iopub.status.idle":"2023-12-16T20:49:53.081612Z","shell.execute_reply.started":"2023-12-16T20:49:29.119243Z","shell.execute_reply":"2023-12-16T20:49:53.080746Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1536dd1167c46dfac1788f6bed0cf8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2dbb6c737274ff7aa57b6df66a5f63e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ba0a5e54ce43038c9c552161fbd4e7"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\nfile_path = '/kaggle/input/complete-dataset/summarized_abstracts_bert/summarized_patient-number2-articles.csv'\ndf = pd.read_csv(file_path)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T20:49:53.083400Z","iopub.execute_input":"2023-12-16T20:49:53.084112Z","iopub.status.idle":"2023-12-16T20:49:53.453844Z","shell.execute_reply.started":"2023-12-16T20:49:53.084078Z","shell.execute_reply":"2023-12-16T20:49:53.452740Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize\nimport pandas as pd\nimport string\n\n# Ensure you have the NLTK punkt tokenizer downloaded in your environment\nnltk.download('punkt')\n\n# Function to split text into sentences and remove punctuation\ndef split_into_sentences(text):\n    return sent_tokenize(text) if pd.notna(text) else []\n\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\n# Load your CSV file\nfile_path = '/kaggle/input/complete-dataset/summarized_abstracts_bert/summarized_patient-number2-articles.csv'  # Update with your file path\ndf = pd.read_csv(file_path)\n\n# Process the summaries to split into sentences and remove punctuation\narticles_sentences_cleaned = []\nfor summary in df['Summary']:\n    if pd.notna(summary):\n        sentences = split_into_sentences(summary)\n        cleaned_sentences = [remove_punctuation(sentence) for sentence in sentences]\n        articles_sentences_cleaned.append(cleaned_sentences)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T20:49:53.455252Z","iopub.execute_input":"2023-12-16T20:49:53.455598Z","iopub.status.idle":"2023-12-16T20:49:55.009653Z","shell.execute_reply.started":"2023-12-16T20:49:53.455567Z","shell.execute_reply":"2023-12-16T20:49:55.008481Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"articles_sentences_cleaned[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-16T20:49:55.012140Z","iopub.execute_input":"2023-12-16T20:49:55.012805Z","iopub.status.idle":"2023-12-16T20:49:55.021998Z","shell.execute_reply.started":"2023-12-16T20:49:55.012762Z","shell.execute_reply":"2023-12-16T20:49:55.020742Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['However the followup coronary angiography showed negative result and the symptom improved dramatically with the treatment of nonsteroidal antiinflammatory drug treatment',\n 'Therefore it is important for the clinician to differentiate acute myocardial infarctionacute stent thrombosis from this rare complication after PCI']"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(0,4):\n    patient_history_sentences = [\n        \"A 65-year-old woman arrives to the ED complaining of chest pain\",\n        \"Her past medical history includes hypertension atherosclerosis, and coronary artery disease\",\n        \"She underwent a coronary artery bypass graft (CABG) 3 weeks ago for three-vessel disease\",\n        \"She reports that her chest pain worsens with inspiration and lessens when leaning forward.\",\n        \"A friction rub is heard on auscultation\",\n        \"ECG shows global ST elevation\"\n    ]\n\n    pubmed_articles = articles_sentences_cleaned[i]\n    # Tokenize sentences\n    tokenized_history = tokenize(patient_history_sentences, tokenizer)\n    tokenized_articles = tokenize(pubmed_articles, tokenizer)\n\n    # Compute similarity scores\n    with torch.no_grad():\n        all_scores = []\n        for i in range(len(tokenized_history['input_ids'])):\n            scores = []\n            for j in range(len(tokenized_articles['input_ids'])):\n                score = model(\n                    input_ids1=tokenized_history['input_ids'][i].unsqueeze(0),\n                    attention_mask1=tokenized_history['attention_mask'][i].unsqueeze(0),\n                    input_ids2=tokenized_articles['input_ids'][j].unsqueeze(0),\n                    attention_mask2=tokenized_articles['attention_mask'][j].unsqueeze(0)\n                )\n                scores.append(score.item())\n            all_scores.append(scores)\n\n    # Aggregate scores (maximum similarity for each sentence in the history list)\n    max_similarity_per_sentence = [max(scores) for scores in all_scores]\n    overall_entropy = sum(max_similarity_per_sentence) / len(max_similarity_per_sentence)\n\n    print(f\"Overall entropy between patient history and pubmed article: {overall_entropy}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-16T20:49:55.023294Z","iopub.execute_input":"2023-12-16T20:49:55.023732Z","iopub.status.idle":"2023-12-16T20:50:05.901894Z","shell.execute_reply.started":"2023-12-16T20:49:55.023685Z","shell.execute_reply":"2023-12-16T20:50:05.900585Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Overall entropy between patient history and pubmed article: 0.09172132052481174\noverall entropy between patient history and pubmed article: 0.06926705564061801\noverall entropy between patient history and pubmed article: 0.08081205499668916\noverall entropy between patient history and pubmed article: 0.027956238637367885\n","output_type":"stream"}]}]}